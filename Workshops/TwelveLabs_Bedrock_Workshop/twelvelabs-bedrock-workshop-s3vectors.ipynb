{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e388aa2",
   "metadata": {},
   "source": [
    "# TwelveLabs on Amazon Bedrock Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea4d77",
   "metadata": {},
   "source": [
    "TwelveLabs is a leading provider of multimodal AI models specializing in video understanding and analysis. TwelveLabs' advanced models enable sophisticated video search, analysis, and content generation capabilities through state-of-the-art computer vision and natural language processing technologies. Amazon Bedrock now offers two TwelveLabs models: TwelveLabs Pegasus 1.2, which provides comprehensive video understanding and analysis, and TwelveLabs Marengo Embed 2.7, which generates high-quality embeddings for video, text, audio, and image content. These models empower developers to build applications that can intelligently process, analyze, and derive insights from video data at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6b3e",
   "metadata": {},
   "source": [
    "### TwelveLabs Video Understanding Models\n",
    "TwelveLabs’ video understanding models consist of a family of deep neural networks built on our multimodal foundation model for video understanding that you can use for the following downstream tasks:\n",
    "- Search using natural language queries\n",
    "- Analyze videos to generate text\n",
    "\n",
    "Videos contain multiple types of information, including visuals, sounds, spoken words, and texts. The human brain combines all types of information and their relations with each other to comprehend the overall meaning of a scene. For example, you’re watching a video of a person jumping and clapping, both visual cues, but the sound is muted. You might realize they’re happy, but you can’t understand why they’re happy without the sound. However, if the sound is unmuted, you could realize they’re cheering for a soccer team that scored a goal.\n",
    "\n",
    "Thus, an application that analyzes a single type of information can’t provide a comprehensive understanding of a video. TwelveLabs’ video understanding models, however, analyze and combine information from all the modalities to accurately interpret the meaning of a video holistically, similar to how humans watch, listen, and read simultaneously to understand videos.\n",
    "\n",
    "Our video understanding models have the ability to identify, analyze, and interpret a variety of elements, including but not limited to the following:\n",
    "| Element | Modality | Example |\n",
    "|---------|----------|---------|\n",
    "| People, including famous individuals | Visual | Michael Jordan, Steve Jobs |\n",
    "| Actions | Visual | Running, dancing, kickboxing |\n",
    "| Objects | Visual | Cars, computers, stadiums |\n",
    "| Animals or pets | Visual | Monkeys, cats, horses |\n",
    "| Nature | Visual | Mountains, lakes, forests |\n",
    "| Text displayed on the screen (OCR) | Visual | License plates, handwritten words, number on a player's jersey |\n",
    "| Brand logos | Visual | Nike, Starbucks, Mercedes |\n",
    "| Shot techniques and effects | Visual | Aerial shots, slow motion, time-lapse |\n",
    "| Counting objects | Visual | Number of people in a crowd, items on a shelf, vehicles in traffic |\n",
    "| Sounds | Audio | Chirping (birds), applause, fireworks popping or exploding |\n",
    "| Human speech | Audio | \"Good morning. How may I help you?\" |\n",
    "| Music | Audio | Ominous music, whistling, lyrics |\n",
    "\n",
    "### Modalities\n",
    "Modalities represent the types of information that the models process and analyze in a video. These modalities are central to both indexing and searching video content.\n",
    "\n",
    "The models support the following modalities: \n",
    "\n",
    "- **Visual**: Analyzes visual content in a video, including actions, objects, events, text (through Optical Character Recognition, or OCR), and brand logos.\n",
    "- **Audio**: Analyzes audio content in a video, including ambient sounds, music, and human speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce5606f",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ac833",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4421157",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff477df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import time\n",
    "import base64\n",
    "from IPython.display import clear_output, HTML, display, Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459abb72",
   "metadata": {},
   "source": [
    "### Configure boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS session\n",
    "session = boto3.Session() # TODO: (optional) replace with your AWS profile, keep as is to use the default profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1758d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AWS Region from session\n",
    "AWS_REGION = session.region_name\n",
    "# AWS_REGION = \"us-east-1\" # OPTIONAL: Manual override for workshop region\n",
    "\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "\n",
    "workshop_supported_regions = [\n",
    "    \"us-east-1\", # N. Virginia\n",
    "    \"eu-west-1\", # Ireland\n",
    "    \"ap-northeast-2\" # Seoul\n",
    "]\n",
    "\n",
    "if AWS_REGION not in workshop_supported_regions:\n",
    "    raise ValueError(f\"AWS Region {AWS_REGION} is not supported for this workshop. Please use one of the following regions: {workshop_supported_regions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS clients\n",
    "bedrock_client = session.client('bedrock-runtime')\n",
    "s3_client = session.client('s3')\n",
    "s3vectors_client = session.client('s3vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3049d4b9",
   "metadata": {},
   "source": [
    "### Configure S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419da591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "S3_BUCKET_NAME = \"<YOUR_S3_BUCKET_NAME>\" # TODO: Replace with your S3 bucket name\n",
    "S3_VIDEOS_PATH = \"videos\"\n",
    "S3_IMAGES_PATH = \"images\"\n",
    "S3_EMBEDDINGS_PATH = \"embeddings\"\n",
    "\n",
    "# Validate S3 bucket name\n",
    "if S3_BUCKET_NAME == \"<YOUR_S3_BUCKET_NAME>\" or S3_BUCKET_NAME == \"\":\n",
    "    raise ValueError(\"Please replace <YOUR_S3_BUCKET_NAME> with your S3 bucket name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c1849",
   "metadata": {},
   "source": [
    "### Configure Amazon S3 Vector bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Vector Bucket Configuration\n",
    "# The S3 Vector Bucket name must be unique name in your AWS account and region\n",
    "# S3 Vector bucket names may only consist of lowercase letters, numbers and hyphens\n",
    "S3_VECTOR_BUCKET_NAME = \"<YOUR_S3_VECTOR_BUCKET_NAME>\" # TODO: Replace with your S3 vector bucket name\n",
    "\n",
    "S3_VECTOR_INDEX_NAME = \"my-vector-index\"\n",
    "\n",
    "# Validate S3 Vector bucket and index names\n",
    "if S3_VECTOR_BUCKET_NAME == \"<YOUR_S3_VECTOR_BUCKET_NAME>\" or S3_VECTOR_BUCKET_NAME == \"\":\n",
    "    raise ValueError(\"Please replace <YOUR_S3_VECTOR_BUCKET_NAME> with your S3 vector bucket name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbed0f",
   "metadata": {},
   "source": [
    "### Bedrock model access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c557f2c",
   "metadata": {},
   "source": [
    "Follow the [Bedrock model access documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) to enable access to TwelveLabs models on Bedrock. Make sure to enable access in the same region you are running this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4e9b3",
   "metadata": {},
   "source": [
    "## Part 1: Multimodal Embeddings with Marengo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db232721",
   "metadata": {},
   "source": [
    "### Part 1a: What is an embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2ec8d",
   "metadata": {},
   "source": [
    "Use TwelveLabs Marengo to create multimodal embeddings for videos, texts, images, and audio files. These embeddings are contextual vector representations (a series of numbers) that capture interactions between modalities, such as visual expressions, body language, spoken words, and video context. You can apply these embeddings to downstream tasks like training custom multimodal models for anomaly detection, diversity sorting, sentiment analysis, recommendations, or building Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "Key features:\n",
    "- **Native multimodal support**: Process all modalities natively without separate models or frame conversion.\n",
    "- **State-of-the-art performance**: Captures motion and temporal information for accurate video interpretation.\n",
    "- **Unified vector space**: Combines embeddings from different modalities for holistic understanding.\n",
    "- **Fast and reliable**: Reduces processing time for large video sets.\n",
    "- **Flexible segmentation**: Generate embeddings for video segments or the entire video.\n",
    "\n",
    "Use cases:\n",
    "- **Anomaly detection**: Identify unusual patterns, such as corrupt videos with black backgrounds, to improve data set quality.\n",
    "- **Diversity sorting**: Organize data for broad representation, reducing bias and improving AI model training.\n",
    "- **Sentiment analysis**: Combine vocal tone, facial expressions, and spoken language for accurate insights, which particularly useful for customer service.\n",
    "- **Recommendations**: Use embeddings in similarity-based retrieval and ranking systems for recommendations.\n",
    "\n",
    "To learn more about embeddings, check out [The Multimodal Evolution of Vector Embeddings](https://www.twelvelabs.io/blog/multimodal-embeddings) on the TwelveLabs Blog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embeddings\n",
    "sample_embedding_1 = np.random.rand(1, 1024)\n",
    "sample_embedding_2 = np.random.rand(1, 1024)\n",
    "\n",
    "df_embedding_1 = pd.DataFrame(sample_embedding_1)\n",
    "df_embedding_2 = pd.DataFrame(sample_embedding_2)\n",
    "\n",
    "df_embedding_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video embedding\n",
    "sample_video_embedding = np.random.rand(5, 1024)\n",
    "df_video_embedding = pd.DataFrame(sample_video_embedding)\n",
    "df_video_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ba741",
   "metadata": {},
   "source": [
    "### Part 1b: Calculating cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6c865",
   "metadata": {},
   "source": [
    "Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between them in high-dimensional space. Unlike distance metrics that consider magnitude, cosine similarity focuses purely on the orientation or direction of vectors, making it particularly useful for comparing text embeddings, documents, and other high-dimensional data.\n",
    "\n",
    "The multimodal vector embeddings from TwelveLabs Marengo can be used to calculate the similarity across text, image, audio, and video.\n",
    "\n",
    "***Formula***\n",
    "\n",
    "The cosine similarity between two vectors **A** and **B** is calculated as:\n",
    "\n",
    "```\n",
    "cos(θ) = (A · B) / (||A|| × ||B||)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **A · B** is the dot product of vectors A and B\n",
    "- **||A||** and **||B||** are the magnitudes (norms) of vectors A and B respectively\n",
    "- **θ** is the angle between the two vectors\n",
    "\n",
    "***Key Characteristics***\n",
    "- **Range**: Values range from -1 to 1\n",
    "  - **1**: Identical direction (perfect similarity)\n",
    "  - **0**: Orthogonal vectors (no similarity)\n",
    "  - **-1**: Opposite directions (perfect dissimilarity)\n",
    "- **Magnitude Independence**: Only considers vector direction, not size\n",
    "- **Symmetric**: cos(A,B) = cos(B,A)\n",
    "\n",
    "***Benefits***\n",
    "- **Scale Invariant**: Ideal for comparing vectors of different magnitudes\n",
    "- **Computationally Efficient**: Fast calculation, especially with sparse vectors\n",
    "- **Robust for Text Analysis**: Perfect for document similarity and text embeddings\n",
    "- **Handles High Dimensions**: Works well in high-dimensional spaces without curse of dimensionality issues\n",
    "- **Intuitive Results**: Easy to interpret similarity scores between 0 and 1 for most applications\n",
    "\n",
    "***Drawbacks***\n",
    "- **Ignores Magnitude**: Completely disregards vector size, which may contain important information\n",
    "- **Limited with Negative Values**: Can be less meaningful when dealing with vectors containing negative components\n",
    "- **Not Always Intuitive**: May not align with human perception of similarity in certain domains\n",
    "- **Loses Information**: Discarding magnitude means losing potentially valuable signal strength data\n",
    "- **Poor for Sparse Positive Data**: May not distinguish well between vectors with very few non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14eac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between two single segment embeddings\n",
    "similarity = cosine_similarity(df_embedding_1, df_embedding_2)\n",
    "pd.DataFrame(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd4020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity with a multi-segment embedding\n",
    "similarities = cosine_similarity(df_video_embedding, df_embedding_1)\n",
    "pd.DataFrame(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615346cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max similarity and the index of the max similarity\n",
    "max_similarity = np.max(similarities)\n",
    "max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "print(f\"Max similarity: {max_similarity}\")\n",
    "print(f\"Index of max similarity: {max_similarity_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f327799",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Building Multimodal Video Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcd427",
   "metadata": {},
   "source": [
    "### Part 2a: Storing videos in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b29da",
   "metadata": {},
   "source": [
    "#### Set up sample dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4987f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Account ID for S3 bucket ownership\n",
    "aws_account_id = session.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"AWS Account ID: {aws_account_id}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET_NAME}\")\n",
    "print(f\"S3 Videos Path: {S3_VIDEOS_PATH}\")\n",
    "print(f\"S3 Images Path: {S3_IMAGES_PATH}\")\n",
    "print(f\"S3 Embeddings Path: {S3_EMBEDDINGS_PATH}\")\n",
    "\n",
    "# Verify bucket access\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=S3_BUCKET_NAME)\n",
    "    print(f\"✅ Successfully connected to S3 bucket: {S3_BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error accessing S3 bucket: {e}\")\n",
    "    print(\"Please ensure the bucket exists and you have proper permissions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719c28d",
   "metadata": {},
   "source": [
    "#### Netflix Open Content\n",
    "\n",
    "The [Netflix Open Content](https://opencontent.netflix.com/) is an open source content available under the [Creative Commons Attribution 4.0 International Public License](https://www.google.com/url?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2Flegalcode&sa=D&sntz=1&usg=AOvVaw3DDX6ldzWtAO5wOs5KkByf).\n",
    "\n",
    "The assets are available for download at: http://download.opencontent.netflix.com/\n",
    "\n",
    "We will be utilizing a subset of the videos for demonstrating how to utilize the TwelveLabs models on Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video S3 URIs\n",
    "sample_videos = [\n",
    "    # 's3://download.opencontent.netflix.com/TechblogAssets/CosmosLaundromat/encodes/CosmosLaundromat_2048x858_24fps_SDR.mp4',\n",
    "    # 's3://download.opencontent.netflix.com/TechblogAssets/Meridian/encodes/Meridian_3840x2160_5994fps_SDR.mp4',\n",
    "    's3://download.opencontent.netflix.com/TechblogAssets/Sparks/encodes/Sparks_4096x2160_5994fps_SDR.mp4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f15db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsigned S3 client\n",
    "public_s3_client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344220f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_s3_uri(s3_uri: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parses an S3 URI like s3://bucket-name/path/to/object and returns (bucket, key)\n",
    "\n",
    "    Args:\n",
    "        s3_uri (str): The S3 URI to parse\n",
    "        \n",
    "    Returns:\n",
    "        tuple[str, str]: The bucket and key\n",
    "    \"\"\"\n",
    "    pattern = r'^s3://([^/]+)/(.+)$'\n",
    "    match = re.match(pattern, s3_uri)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid S3 URI format: {s3_uri}\")\n",
    "    return match.group(1), match.group(2)\n",
    "\n",
    "\n",
    "def copy_public_s3_object_to_private_bucket(public_s3_uri: str, dest_bucket: str, dest_key: str, aws_profile: str = 'default') -> None:\n",
    "    \"\"\"\n",
    "    Copies a public S3 object to a private bucket\n",
    "\n",
    "    Args:\n",
    "        public_s3_uri (str): The S3 URI of the public object to copy\n",
    "        dest_bucket (str): The name of the private bucket to copy to\n",
    "        dest_key (str): The key of the object to copy to\n",
    "        aws_profile (str): The AWS profile to use for the authenticated client\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse source bucket and key\n",
    "    source_bucket, source_key = parse_s3_uri(public_s3_uri)\n",
    "\n",
    "    # Anonymous client to read public object\n",
    "    anon_s3 = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n",
    "\n",
    "    print(f\"Downloading from {public_s3_uri}...\")\n",
    "    response = anon_s3.get_object(Bucket=source_bucket, Key=source_key)\n",
    "    data = response['Body'].read()\n",
    "\n",
    "    print(f\"Uploading to s3://{dest_bucket}/{dest_key} ...\")\n",
    "    s3_client.put_object(Bucket=dest_bucket, Key=dest_key, Body=data)\n",
    "\n",
    "    print(\"✅ Copy completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81effe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy videos to the S3 bucket\n",
    "for video_uri in sample_videos:\n",
    "    # Extract the filename from the S3 key\n",
    "    _, src_key = parse_s3_uri(video_uri)\n",
    "    filename = src_key.split(\"/\")[-1]\n",
    "    dest_key = f\"{S3_VIDEOS_PATH}/{filename}\"\n",
    "    copy_public_s3_object_to_private_bucket(\n",
    "        public_s3_uri=video_uri,\n",
    "        dest_bucket=S3_BUCKET_NAME,\n",
    "        dest_key=dest_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b23db",
   "metadata": {},
   "source": [
    "### Part 2b: Creating vector embeddings with Marengo on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc1fc68",
   "metadata": {},
   "source": [
    "#### TwelveLabs Marengo\n",
    "\n",
    "Marengo is an embedding model for comprehensive video understanding. Marengo analyzes multiple modalities in video content, including visuals, audio, and text, to provide a holistic understanding similar to human comprehension.\n",
    "\n",
    "***Key features***\n",
    "- **Multimodal processing:** Combines visual, audio, and text elements for comprehensive understanding\n",
    "- **Fine-grained search:** Detects brand logos, text, and small objects (as small as 10% of the video frame)\n",
    "- **Motion search:** Identifies and analyzes movement within videos\n",
    "- **Counting capabilities:** Accurately counts objects in video frames\n",
    "- **Audio comprehension:** Analyzes music, lyrics, sound, and silence\n",
    "\n",
    "***Use cases***\n",
    "- **Search:** Use natural language queries to find specific content within videos\n",
    "- **Embeddings:** Create video embeddings for various downstream applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d5cb3f",
   "metadata": {},
   "source": [
    "#### Marengo Embed 2.7 on Bedrock\n",
    "\n",
    "A multimodal embedding model that generates high-quality vector representations of video, text, audio, and image content for similarity search, clustering, and other machine learning tasks. The model supports multiple input modalities and provides specialized embeddings optimized for different use cases.\n",
    "\n",
    "The model supports synchronous inference through the [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) and asynchronous inference through the [StartAsyncInvoke API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_StartAsyncInvoke.html).\n",
    "- Provider — TwelveLabs\n",
    "- Categories — Embeddings, multimodal\n",
    "- Model ID — `twelvelabs.marengo-embed-2-7-v1:0`\n",
    "- Input modality — Video, Text, Audio, Image\n",
    "- Output modality — Embeddings\n",
    "- Max video size — 2 hours long video (< 2GB file size)\n",
    "\n",
    "| API operation | Supported model types | Input modalities | Output modalities |\n",
    "|---------------|-----------------------|------------------|-------------------|\n",
    "| InvokeModel | [Inference profiles](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-profiles-support.html) | Text, Image | Embedding |\n",
    "| StartAsyncInvoke | [Base models](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) | Video, Audio, Image, Text | Embedding |\n",
    "\n",
    "**Resources:**\n",
    "- [AWS Docs: TwelveLabs Marengo Embed 2.7](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-marengo.html)\n",
    "- [TwelveLabs Docs: Marengo](https://docs.twelvelabs.io/v1.3/docs/concepts/models/marengo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marengo model configuration\n",
    "MARENGO_MODEL_ID = 'twelvelabs.marengo-embed-2-7-v1:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e7a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARENGO_INFERENCE_ID_REGIONS = {\n",
    "    \"us-east-1\": \"us.twelvelabs.marengo-embed-2-7-v1:0\",\n",
    "    \"eu-west-1\": \"eu.twelvelabs.marengo-embed-2-7-v1:0\",\n",
    "    \"ap-northeast-2\": \"apac.twelvelabs.marengo-embed-2-7-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    MARENGO_INFERENCE_ID = MARENGO_INFERENCE_ID_REGIONS[AWS_REGION]\n",
    "    print(MARENGO_INFERENCE_ID)\n",
    "except KeyError:\n",
    "    raise ValueError(f\"Marengo is not supported for {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a7819",
   "metadata": {},
   "source": [
    "##### Creating a text embedding with Marengo InvokeModel API\n",
    "\n",
    "Marengo Embed 2.7 supports synchronous invocation for text and image inputs using the [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) on Amazon Bedrock.\n",
    "\n",
    "**Text** input for the InvokeModel API can be used with the following model input:\n",
    "```\n",
    "model_input = { \n",
    "    \"inputType\": \"text\",\n",
    "    \"inputText\": text_query\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf766f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to create text embedding\n",
    "def create_text_embedding(text_query: str) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for text using Marengo on Bedrock\n",
    "\n",
    "    Args:\n",
    "        text_query (str): The text query to create an embedding for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "    \"\"\"\n",
    "    \n",
    "    model_input = { \n",
    "        \"inputType\": \"text\",\n",
    "        \"inputText\": text_query\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=MARENGO_INFERENCE_ID,\n",
    "        body=json.dumps(model_input)\n",
    "    )\n",
    "    \n",
    "    embedding_data = json.loads(response['body'].read().decode('utf-8'))['data']\n",
    "    \n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create text embedding\n",
    "text_query = \"two people having a conversation in a car\"\n",
    "\n",
    "print(f\"Creating text embedding for query\")\n",
    "text_embedding_data = create_text_embedding(text_query)\n",
    "\n",
    "print(f\"✅ Text embedding created successfully with {len(text_embedding_data)} segment and {len(text_embedding_data[0]['embedding'])} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c8c4a",
   "metadata": {},
   "source": [
    "##### Creating an image embedding with Marengo InvokeModel API\n",
    "\n",
    "**Image** input for the InvokeModel API can be defined as a Base64-encoded string or as an S3 location.\n",
    "\n",
    "1. Base64-encoded string\n",
    "```\n",
    "    {\n",
    "        \"mediaSource\": {\n",
    "            \"base64String\": \"base64-encoded string\"\n",
    "        }\n",
    "    }\n",
    "```\n",
    "- `base64String` – The Base64-encoded string for the media.\n",
    "\n",
    "2. S3 location – Specify the S3 URI and the\n",
    "```\n",
    "    {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": \"string\",\n",
    "            \"bucketOwner\": \"string\"\n",
    "        }\n",
    "    }\n",
    "```\n",
    "- `uri` – The S3 URI containing the media.\n",
    "- `bucketOwner` – The AWS account ID of the S3 bucket owner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image\n",
    "image_path = \"assets/images/image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to create image embedding\n",
    "def create_image_embedding(image_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for image using Marengo on Bedrock\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the image to create an embedding for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'^s3://([^/]+)/(.+)$'\n",
    "    match = re.match(pattern, image_path)\n",
    "    if match:\n",
    "        # image is located on S3\n",
    "        media_source = {\n",
    "            \"s3Location\": {\n",
    "                \"uri\": image_path,\n",
    "                \"bucketOwner\": aws_account_id\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # image is a local file path\n",
    "        media_source = {\n",
    "            \"base64String\": base64.b64encode(open(image_path, \"rb\").read()).decode('utf-8')\n",
    "        }\n",
    "\n",
    "    model_input = { \n",
    "        \"inputType\": \"image\",\n",
    "        \"mediaSource\": media_source\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        modelId=MARENGO_INFERENCE_ID,\n",
    "        body=json.dumps(model_input)\n",
    "    )\n",
    "    \n",
    "    embedding_data = json.loads(response['body'].read().decode('utf-8'))['data']\n",
    "    \n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create image embedding\n",
    "print(f\"Creating embeddings for image at {image_path}\")\n",
    "image_embedding_data = create_image_embedding(image_path)\n",
    "\n",
    "print(f\"✅ Image embedding created successfully with {len(image_embedding_data)} segment(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae33fdf8",
   "metadata": {},
   "source": [
    "##### Creating a video embedding with Marengo StartAsyncInvoke API\n",
    "\n",
    "**Video** and **audio** inputs can be processed by Marengo with the [StartAsyncInvoke API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_StartAsyncInvoke.html). The model outputs will land in the S3 location specified by `outputDataConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727c0ff",
   "metadata": {},
   "source": [
    "Since the StartAsyncInvoke API asynchronously executes the task, the helper function below triggers the task and waits for it to complete. It then retrieves the outputs from the output S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to wait for async embedding results\n",
    "def wait_for_embedding_output(s3_bucket: str, s3_prefix: str, invocation_arn: str, verbose: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Wait for Bedrock async embedding task to complete and retrieve results\n",
    "\n",
    "    Args:\n",
    "        s3_bucket (str): The S3 bucket name\n",
    "        s3_prefix (str): The S3 prefix for the embeddings\n",
    "        invocation_arn (str): The ARN of the Bedrock async embedding task\n",
    "\n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If the embedding task fails or no output.json is found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wait until task completes\n",
    "    status = None\n",
    "    while status not in [\"Completed\", \"Failed\", \"Expired\"]:\n",
    "        response = bedrock_client.get_async_invoke(invocationArn=invocation_arn)\n",
    "        status = response['status']\n",
    "        if verbose:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Embedding task status: {status}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    if status != \"Completed\":\n",
    "        raise Exception(f\"Embedding task failed with status: {status}\")\n",
    "    \n",
    "    # Retrieve the output from S3\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        if obj['Key'].endswith('output.json'):\n",
    "            output_key = obj['Key']\n",
    "            obj = s3_client.get_object(Bucket=s3_bucket, Key=output_key)\n",
    "            content = obj['Body'].read().decode('utf-8')\n",
    "            data = json.loads(content).get(\"data\", [])\n",
    "            return data\n",
    "    \n",
    "    raise Exception(\"No output.json found in S3 prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae74784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function to create and retrieve video embeddings\n",
    "def create_video_embedding(video_s3_uri: str) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for video using Marengo on Bedrock\n",
    "    \n",
    "    Args:\n",
    "        video_s3_uri (str): The S3 URI of the video to create an embedding for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_id = uuid.uuid4()\n",
    "    s3_output_prefix = f'{S3_EMBEDDINGS_PATH}/{S3_VIDEOS_PATH}/{unique_id}'\n",
    "    \n",
    "    response = bedrock_client.start_async_invoke(\n",
    "        modelId=MARENGO_MODEL_ID,\n",
    "        modelInput={\n",
    "            \"inputType\": \"video\",\n",
    "            \"mediaSource\": {\n",
    "                \"s3Location\": {\n",
    "                    \"uri\": video_s3_uri,\n",
    "                    \"bucketOwner\": aws_account_id\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f's3://{S3_BUCKET_NAME}/{s3_output_prefix}'\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    invocation_arn = response[\"invocationArn\"]\n",
    "    print(f\"Video embedding task started: {invocation_arn}\")\n",
    "    \n",
    "    # Wait for completion and get results\n",
    "    try:\n",
    "        embedding_data = wait_for_embedding_output(S3_BUCKET_NAME, s3_output_prefix, invocation_arn)\n",
    "    except Exception as e:\n",
    "        print(f\"Error waiting for embedding output: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return embedding_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create video embeddings\n",
    "videos = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_VIDEOS_PATH)[\"Contents\"]\n",
    "video_uri = f\"s3://{S3_BUCKET_NAME}/{videos[0]['Key']}\"\n",
    "\n",
    "print(f\"Creating embeddings for video: {video_uri}\")\n",
    "video_embedding_data = create_video_embedding(video_uri)\n",
    "\n",
    "print(f\"✅ Video embedding created successfully with {len(video_embedding_data)} segment(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f86a1",
   "metadata": {},
   "source": [
    "### Part 2c: Creating a vector index in Amazon S3 Vectors\n",
    "\n",
    "Amazon S3 Vectors is an Amazon Simple Storage Service (S3) feature designed for storing and querying large collections of vector embeddings. It's a purpose-built, durable vector store that integrates vector data directly into the S3 object storage infrastructure, enabling sub-second query performance on vector embeddings. For more information, please refer to the [Amazon S3 Vectors User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9b61f",
   "metadata": {},
   "source": [
    "#### Configure Amazon S3 Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36447360",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s3vectors_client.get_vector_bucket(vectorBucketName=S3_VECTOR_BUCKET_NAME)\n",
    "    print(f'S3 Vector Bucket {S3_VECTOR_BUCKET_NAME} already exists')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # bucket does not exist, create it\n",
    "    s3vectors_client.create_vector_bucket(vectorBucketName=S3_VECTOR_BUCKET_NAME)\n",
    "\n",
    "try:\n",
    "    s3vectors_client.get_index(vectorBucketName=S3_VECTOR_BUCKET_NAME, indexName=S3_VECTOR_INDEX_NAME)\n",
    "    print(f'Amazon S3 Vector Index {S3_VECTOR_INDEX_NAME} already exists')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    # Twelvelabs Marengo embeddings are 1024 dimensions with 32-bit floating point\n",
    "    s3vectors_client.create_index(vectorBucketName=S3_VECTOR_BUCKET_NAME, \n",
    "        indexName=S3_VECTOR_INDEX_NAME,\n",
    "        dataType='float32',\n",
    "        dimension=1024,\n",
    "        distanceMetric='cosine')\n",
    "\n",
    "print(f'S3 Vector Bucket {S3_VECTOR_BUCKET_NAME} and Index {S3_VECTOR_INDEX_NAME} ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2883f",
   "metadata": {},
   "source": [
    "#### Bulk process videos in S3 with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb545e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_video_embeddings(s3_vector_bucket: str, s3_vector_index: str, video_s3_uri: str, video_embeddings: list) -> int:\n",
    "    \"\"\" \n",
    "    Index video embeddings in S3 Vector Index \n",
    "   \n",
    "     Args:\n",
    "        s3_vector_bucket (str): The name of the bucket to use\n",
    "        s3_vector_index (str): The name of the index to use\n",
    "        video_embeddings (list): The list of video embeddings\n",
    "\n",
    "    Returns:\n",
    "        int: The number of documents indexed\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for ve in video_embeddings:\n",
    "        embeddings.append({\n",
    "            \"key\": f'{ve[\"embeddingOption\"]} {ve[\"startSec\"]} {ve[\"endSec\"]}',\n",
    "            \"data\": {\"float32\": ve[\"embedding\"]},\n",
    "            \"metadata\": {\n",
    "                \"embeddingOption\": ve[\"embeddingOption\"], \n",
    "                \"startSec\": ve[\"startSec\"], \n",
    "                \"endSec\": ve[\"endSec\"],\n",
    "                \"video_s3_uri\": video_s3_uri,\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Write embeddings into vector index with metadata.\n",
    "    s3vectors_client.put_vectors(\n",
    "        vectorBucketName=s3_vector_bucket,   \n",
    "        indexName=s3_vector_index,   \n",
    "        vectors=embeddings\n",
    "    )\n",
    "\n",
    "    return len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing video embeddings in S3 bucket\n",
    "response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=f\"{S3_EMBEDDINGS_PATH}/{S3_VIDEOS_PATH}\")\n",
    "\n",
    "# Empty video embeddings in S3 bucket\n",
    "try:\n",
    "    if 'Contents' in response:\n",
    "        objects_to_delete = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "        s3_client.delete_objects(\n",
    "            Bucket=S3_BUCKET_NAME,\n",
    "            Delete={'Objects': objects_to_delete}\n",
    "        )\n",
    "        print(f\"✅ Removed existing video embeddings successfully.\")\n",
    "    else:\n",
    "        print(f\"✅ No existing video embeddings found.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error emptying video embeddings: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87663e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of videos in the s3 bucket and loop through them to create embeddings\n",
    "videos = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_VIDEOS_PATH)[\"Contents\"]\n",
    "\n",
    "video_embeddings = []\n",
    "\n",
    "for video in videos:\n",
    "    video_uri = f\"s3://{S3_BUCKET_NAME}/{video['Key']}\"\n",
    "    print(f\"Creating embeddings for video: {video_uri}\")\n",
    "    results = create_video_embedding(video_uri)\n",
    "\n",
    "    print(f\"✅ Video embedding created successfully with {len(results)} segment(s) from {video['Key']}\")\n",
    "\n",
    "    video_embeddings.append({\n",
    "        \"video_s3_uri\": video_uri,\n",
    "        \"embeddings_data\": results\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ab091",
   "metadata": {},
   "source": [
    "#### Insert embeddings into S3 Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_embedding in video_embeddings:\n",
    "\n",
    "    # Use the index_video_embeddings function to index the embedding data into Amazon S3 vectors\n",
    "    num_indexed = index_video_embeddings(S3_VECTOR_BUCKET_NAME, S3_VECTOR_INDEX_NAME, video_embedding['video_s3_uri'], video_embedding['embeddings_data'])\n",
    "\n",
    "    print(f\"✅ Indexed {len(video_embedding['embeddings_data'])} segments from {video_embedding['video_s3_uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c4d45",
   "metadata": {},
   "source": [
    "### Part 2d: Querying for multimodal video search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to play a video at a specific start time\n",
    "def play_video(video_url: str, start_time: float) -> None:\n",
    "    \"\"\"\n",
    "    Play a video at a specific start time.\n",
    "\n",
    "    Args:\n",
    "        video_url (str): The URL of the video to play.\n",
    "        start_time (float): The start time of the video in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    # HTML code for the video player\n",
    "    html_code = f\"\"\"\n",
    "    <video width=\"640\" controls>\n",
    "        <source src=\"{video_url}#t={start_time}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\"\n",
    "    display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9fc16e",
   "metadata": {},
   "source": [
    "#### Query with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a54a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Query Search Function\n",
    "def search_videos_by_text(query_text: str, top_k: int=5) -> list:\n",
    "    \"\"\"\n",
    "    Search for video segments using text queries\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The text query to search for.\n",
    "        top_k (int): The number of videos to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of video segments that match the query.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the text query\n",
    "    print(f\"Generating embedding for query: '{query_text}'\")\n",
    "    query_embedding_data = create_text_embedding(query_text)\n",
    "    query_embedding = query_embedding_data[0][\"embedding\"]\n",
    "\n",
    "    # Search S3 Vector Index\n",
    "    response = s3vectors_client.query_vectors(\n",
    "        vectorBucketName=S3_VECTOR_BUCKET_NAME,\n",
    "        indexName=S3_VECTOR_INDEX_NAME,\n",
    "        queryVector={\"float32\": query_embedding}, \n",
    "        topK=top_k, \n",
    "        returnDistance=True,\n",
    "        returnMetadata=True\n",
    "    )\n",
    "    print(json.dumps(response[\"vectors\"], indent=2))\n",
    "\n",
    "    print(f\"\\n✅ Found {len(response['vectors'])} matching segments:\")\n",
    "    results = []\n",
    "    \n",
    "    for hit in response['vectors']:\n",
    "        result = {\n",
    "            \"distance\": hit[\"distance\"],\n",
    "            \"video_s3_uri\": hit[\"metadata\"]['video_s3_uri'],\n",
    "            \"start_time\": hit[\"metadata\"][\"startSec\"],\n",
    "            \"end_time\": hit[\"metadata\"][\"endSec\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Score: {result['distance']:.4f} | \"\n",
    "              f\"Video : {result['video_s3_uri']} | Time: {result['start_time']:.1f}s - {result['end_time']:.1f}s\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"a person wearing safety gear and welding with a forest in the background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55305e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text search\n",
    "text_search_results = search_videos_by_text(text_query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top result\n",
    "top_text_result = text_search_results[0]\n",
    "video_bucket, video_key = parse_s3_uri(top_text_result[\"video_s3_uri\"])\n",
    "\n",
    "# Generate presigned URL for the video\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    \"get_object\",\n",
    "    Params={\"Bucket\": video_bucket, \"Key\": video_key},\n",
    "    ExpiresIn=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the video stream URL and the start time\n",
    "video_url = presigned_url\n",
    "start_time = top_text_result[\"start_time\"]\n",
    "print(f\"\\nVideo URL: {video_url}\")\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "# Play the video\n",
    "play_video(video_url, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6ef0e",
   "metadata": {},
   "source": [
    "#### Query with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Query Search Function\n",
    "def search_videos_by_image(image_path: str, top_k: int=5) -> list:\n",
    "    \"\"\"\n",
    "    Search for videos that contain the given image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image to search for.\n",
    "        top_k (int): The number of videos to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of video segments that match the query.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the image\n",
    "    print(f\"Generating embedding for query: '{image_path}'\")\n",
    "    query_embedding_data = create_image_embedding(image_path)\n",
    "    query_embedding = query_embedding_data[0][\"embedding\"]\n",
    "\n",
    "    # Search S3 Vector Index\n",
    "    response = s3vectors_client.query_vectors(\n",
    "        vectorBucketName=S3_VECTOR_BUCKET_NAME,\n",
    "        indexName=S3_VECTOR_INDEX_NAME,\n",
    "        queryVector={\"float32\": query_embedding}, \n",
    "        topK=top_k, \n",
    "        returnDistance=True,\n",
    "        returnMetadata=True\n",
    "    )\n",
    "    print(json.dumps(response[\"vectors\"], indent=2))\n",
    "\n",
    "    print(f\"\\n✅ Found {len(response['vectors'])} matching segments:\")\n",
    "    results = []\n",
    "    \n",
    "    for hit in response['vectors']:\n",
    "        result = {\n",
    "            \"distance\": hit[\"distance\"],\n",
    "            \"video_s3_uri\": hit[\"metadata\"]['video_s3_uri'],\n",
    "            \"start_time\": hit[\"metadata\"][\"startSec\"],\n",
    "            \"end_time\": hit[\"metadata\"][\"endSec\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Score: {result['distance']:.4f} | \"\n",
    "              f\"Video : {result['video_s3_uri']} | Time: {result['start_time']:.1f}s - {result['end_time']:.1f}s\")\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91918ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_query = \"assets/images/image.jpg\"\n",
    "\n",
    "display(Image(filename=image_query, width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image search\n",
    "image_search_results = search_videos_by_image(image_path=image_query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffae20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top result\n",
    "top_image_result = image_search_results[0]\n",
    "video_bucket, video_key = parse_s3_uri(top_image_result[\"video_s3_uri\"])\n",
    "\n",
    "# Generate presigned URL for the video\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    \"get_object\",\n",
    "    Params={\"Bucket\": video_bucket, \"Key\": video_key},\n",
    "    ExpiresIn=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1569cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the video stream URL and the start time\n",
    "video_url = presigned_url\n",
    "start_time = top_image_result[\"start_time\"]\n",
    "print(f\"\\nVideo URL: {video_url}\")\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "# Play the video\n",
    "play_video(video_url, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22f905",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99208e0e",
   "metadata": {},
   "source": [
    "## Part 3: Using Pegasus on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba775a",
   "metadata": {},
   "source": [
    "### Bedrock model access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38c6c6",
   "metadata": {},
   "source": [
    "Follow the [Bedrock model access documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html) to enable access to TwelveLabs models on Bedrock. Make sure to enable access in the same region you are running this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEGASUS_MODEL_ID_REGIONS = {\n",
    "    \"us-east-1\": \"us.twelvelabs.pegasus-1-2-v1:0\",\n",
    "    \"us-west-2\": \"us.twelvelabs.pegasus-1-2-v1:0\",\n",
    "    \"eu-west-1\": \"eu.twelvelabs.pegasus-1-2-v1:0\",\n",
    "    \"ap-northeast-2\": \"apac.twelvelabs.pegasus-1-2-v1:0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4e1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    PEGASUS_MODEL_ID = PEGASUS_MODEL_ID_REGIONS[AWS_REGION]\n",
    "    print(PEGASUS_MODEL_ID)\n",
    "except KeyError:\n",
    "    raise ValueError(f\"Pegasus 1.2 is not supported for {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e203f",
   "metadata": {},
   "source": [
    "### Select the video to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a38405",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_VIDEOS_PATH)\n",
    "\n",
    "# List all object keys\n",
    "if 'Contents' in s3_response:\n",
    "    object_keys = [obj['Key'] for obj in s3_response['Contents']]\n",
    "    for key in object_keys:\n",
    "        print(key)\n",
    "    print(f\"\\nTotal objects found: {len(object_keys)}\")\n",
    "else:\n",
    "    print(\"No objects found in the specified bucket and prefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9806c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_s3_key = \"videos/Sparks_4096x2160_5994fps_SDR.mp4\" # TODO: Replace with your video S3 key\n",
    "\n",
    "# Validate video S3 key\n",
    "if video_s3_key == \"<YOUR_VIDEO_S3_KEY>\" or video_s3_key == \"\":\n",
    "    raise ValueError(\"Please replace <YOUR_VIDEO_S3_KEY> with your video S3 key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0253a3c",
   "metadata": {},
   "source": [
    "### View the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24805bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate presigned URL for the video\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    \"get_object\",\n",
    "    Params={\"Bucket\": S3_BUCKET_NAME, \"Key\": video_s3_key},\n",
    "    ExpiresIn=3600\n",
    ")\n",
    "\n",
    "# Play the video\n",
    "play_video(presigned_url, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cff7f3",
   "metadata": {},
   "source": [
    "### Part 3a: Analyze with Pegasus on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211ded1",
   "metadata": {},
   "source": [
    "#### TwelveLabs Pegasus\n",
    "\n",
    "Pegasus is a generative model for video-to-text generation. Pegasus analyzes multiple modalities to generate contextually relevant text based on the content of your videos.\n",
    "\n",
    "***Key features***\n",
    "- **Video-to-text generation**: Creates detailed textual descriptions based on video content\n",
    "- **Extended processing capacity**: Processes videos up to 1 hour in length\n",
    "- **Granular visual comprehension**: Analyzes objects, on-screen text, and numerical content\n",
    "- **Temporal grounding**: Accurately identifies timestamps of specific events\n",
    "- **Multimodal understanding**: Combines visual, audio, and textual information for comprehensive analysis\n",
    "\n",
    "***Use cases***\n",
    "- **Content summarization**: Generate concise summaries of video content\n",
    "- **Detailed descriptions**: Create comprehensive textual descriptions of visual scenes\n",
    "- **Timestamp identification**: Answer questions about when specific events occur in videos\n",
    "- **Content analysis**: Extract key information from video content for further processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10564fe",
   "metadata": {},
   "source": [
    "#### Pegasus 1.2 on Bedrock\n",
    "\n",
    "The TwelveLabs Pegasus 1.2 model provides comprehensive video understanding and analysis capabilities. It can analyze video content and generate textual descriptions, insights, and answers to questions about the video.\n",
    "\n",
    "Use this information to make inference calls to TwelveLabs models with the [InvokeModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html), [InvokeModelWithResponseStream](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModelWithResponseStream.html) (streaming) operations.\n",
    "\n",
    "- Provider — TwelveLabs\n",
    "- Categories — Video understanding, content analysis\n",
    "- Model ID — `twelvelabs.pegasus-1-2-v1:0`\n",
    "- Input modality — Video\n",
    "- Output modality — Text\n",
    "- Max video size — 1 hour long video (< 2GB file size)\n",
    "\n",
    "**Resources:**\n",
    "- [AWS Docs: TwelveLabs Pegasus 1.2](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-pegasus.html)\n",
    "- [TwelveLabs Docs: Pegasus](https://docs.twelvelabs.io/v1.3/docs/concepts/models/pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the video about?\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputPrompt\": prompt,\n",
    "    \"mediaSource\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{S3_BUCKET_NAME}/{video_s3_key}\",\n",
    "            \"bucketOwner\": aws_account_id\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "response = bedrock_client.invoke_model(\n",
    "    modelId=PEGASUS_MODEL_ID,\n",
    "    body=json.dumps(request_body),\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the response and print the model outputs\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "print(response_body[\"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e9565",
   "metadata": {},
   "source": [
    "### Part 3b: Pegasus streaming response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the video about?\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputPrompt\": prompt,\n",
    "    \"mediaSource\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{S3_BUCKET_NAME}/{video_s3_key}\",\n",
    "            \"bucketOwner\": aws_account_id\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "streaming_response = bedrock_client.invoke_model_with_response_stream(\n",
    "    modelId=PEGASUS_MODEL_ID,\n",
    "    body=json.dumps(request_body),\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "message = \"\"\n",
    "for event in streaming_response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    print(chunk[\"message\"], end=\"\")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95fb43",
   "metadata": {},
   "source": [
    "## Part 4: Video Analysis with Pegasus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37b8ac",
   "metadata": {},
   "source": [
    "Pegasus analyzes videos to generate text based on their content using a multimodal approach. This method analyzes the visuals, sounds, spoken words, and relationships between them. As a result, it provides a comprehensive understanding of your videos, capturing nuances that might be overlooked when using an unimodal interpretation.\n",
    "\n",
    "The platform generates the following types of text:\n",
    "- **Topics and hashtags:** Represent a swift breakdown of the essence of a video.\n",
    "- **Summaries:** Encapsulate the key points of a video, presenting the most important information clearly and concisely.\n",
    "- **Highlights:** List the key events in order. Unlike chapters, they spotlight primary topics.\n",
    "- **Chapters:** A chapter in a video typically focuses on a particular topic or theme. The platform chronologically lists all the chapters in your video for a thorough content breakdown.\n",
    "- **Open-ended text (your own prompt):** Custom outputs based on your prompts, including, but not limited to, tables of content, action items, memos, reports, marketing copy, and comprehensive analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475034c5",
   "metadata": {},
   "source": [
    "### Part 4a: Summaries, hashtags, and highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the video\n",
    "prompt = \"Summarize the video\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputPrompt\": prompt,\n",
    "    \"mediaSource\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{S3_BUCKET_NAME}/{video_s3_key}\",\n",
    "            \"bucketOwner\": aws_account_id\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "streaming_response = bedrock_client.invoke_model_with_response_stream(\n",
    "    modelId=PEGASUS_MODEL_ID,\n",
    "    body=json.dumps(request_body),\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "message = \"\"\n",
    "for event in streaming_response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    print(chunk[\"message\"], end=\"\")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relevant hashtags for the video\n",
    "prompt = \"Generate hashtags for the video\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputPrompt\": prompt,\n",
    "    \"mediaSource\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{S3_BUCKET_NAME}/{video_s3_key}\",\n",
    "            \"bucketOwner\": aws_account_id\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "streaming_response = bedrock_client.invoke_model_with_response_stream(\n",
    "    modelId=PEGASUS_MODEL_ID,\n",
    "    body=json.dumps(request_body),\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "message = \"\"\n",
    "for event in streaming_response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    print(chunk[\"message\"], end=\"\")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447272ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate highlights of the video\n",
    "prompt = \"What are the highlighted moments of this video?\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputPrompt\": prompt,\n",
    "    \"mediaSource\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{S3_BUCKET_NAME}/{video_s3_key}\",\n",
    "            \"bucketOwner\": aws_account_id\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "streaming_response = bedrock_client.invoke_model_with_response_stream(\n",
    "    modelId=PEGASUS_MODEL_ID,\n",
    "    body=json.dumps(request_body),\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "message = \"\"\n",
    "for event in streaming_response[\"body\"]:\n",
    "    chunk = json.loads(event[\"chunk\"][\"bytes\"])\n",
    "    print(chunk[\"message\"], end=\"\")\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea5436",
   "metadata": {},
   "source": [
    "### Part 4b: Structured outputs\n",
    "\n",
    "Structured outputs for Pegasus lets users specify the structured output format as a JSON schema. Structured outputs can be useful for building automated integrations to application workflows such as metadata enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19274693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using JSON Schema to generate structured output\n",
    "prompt = \"\"\"\n",
    "Generate metadata for the video with the following fields:\n",
    "- title: (string) The title of the video\n",
    "- description: (string) The description of the video\n",
    "- mood: (string) The mood of the video\n",
    "- genre: (string) The genre of the video\n",
    "\"\"\"\n",
    "\n",
    "request_body = {\n",
    "    \"inputPrompt\": prompt,\n",
    "    \"mediaSource\": {\n",
    "        \"s3Location\": {\n",
    "            \"uri\": f\"s3://{S3_BUCKET_NAME}/{video_s3_key}\",\n",
    "            \"bucketOwner\": aws_account_id\n",
    "        }\n",
    "    },\n",
    "    \"temperature\": 0,\n",
    "    \"responseFormat\": {\n",
    "        \"jsonSchema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"description\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"mood\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"genre\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"title\", \"description\", \"mood\", \"genre\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = bedrock_client.invoke_model(\n",
    "    modelId=PEGASUS_MODEL_ID,\n",
    "    body=json.dumps(request_body),\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the response and print the model outputs\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "message_data = json.loads(response_body[\"message\"])\n",
    "print(json.dumps(message_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1aa595",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74a23c",
   "metadata": {},
   "source": [
    "#### Delete S3 Vector Bucket and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d7b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3vectors_client.list_indexes(\n",
    "    vectorBucketName=S3_VECTOR_BUCKET_NAME\n",
    ")\n",
    "\n",
    "if 'indexes' in response:\n",
    "    for index in response['indexes']:\n",
    "        s3vectors_client.delete_index(\n",
    "            vectorBucketName=S3_VECTOR_BUCKET_NAME,\n",
    "            indexName=index['indexName']\n",
    "        )\n",
    "\n",
    "s3vectors_client.delete_vector_bucket(vectorBucketName=S3_VECTOR_BUCKET_NAME)\n",
    "\n",
    "print(f\"S3 Vector Bucket {S3_VECTOR_BUCKET_NAME} removed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41eee6",
   "metadata": {},
   "source": [
    "#### Empty S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2dbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List objects and prepare for deletion\n",
    "response = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME)\n",
    "\n",
    "# Empty S3 bucket\n",
    "try:\n",
    "    if 'Contents' in response:\n",
    "        objects_to_delete = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "        s3_client.delete_objects(\n",
    "            Bucket=S3_BUCKET_NAME,\n",
    "            Delete={'Objects': objects_to_delete}\n",
    "        )\n",
    "        print(f\"✅ Bucket '{S3_BUCKET_NAME}' emptied successfully.\")\n",
    "    else:\n",
    "        print(f\"✅ Bucket '{S3_BUCKET_NAME}' is already empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error emptying bucket '{S3_BUCKET_NAME}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96636901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
