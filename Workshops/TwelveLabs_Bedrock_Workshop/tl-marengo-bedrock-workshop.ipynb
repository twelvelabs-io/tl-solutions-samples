{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e388aa2",
   "metadata": {},
   "source": [
    "# TwelveLabs Marengo on Amazon Bedrock Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea4d77",
   "metadata": {},
   "source": [
    "TwelveLabs is a leading provider of multimodal AI models specializing in video understanding and analysis. TwelveLabs' advanced models enable sophisticated video search, analysis, and content generation capabilities through state-of-the-art computer vision and natural language processing technologies. Amazon Bedrock now offers two TwelveLabs models: TwelveLabs Pegasus 1.2, which provides comprehensive video understanding and analysis, and TwelveLabs Marengo Embed 2.7, which generates high-quality embeddings for video, text, audio, and image content. These models empower developers to build applications that can intelligently process, analyze, and derive insights from video data at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e6b3e",
   "metadata": {},
   "source": [
    "### TwelveLabs Video Understanding Models\n",
    "TwelveLabs’ video understanding models consist of a family of deep neural networks built on our multimodal foundation model for video understanding that you can use for the following downstream tasks:\n",
    "- Search using natural language queries\n",
    "- Analyze videos to generate text\n",
    "\n",
    "Videos contain multiple types of information, including visuals, sounds, spoken words, and texts. The human brain combines all types of information and their relations with each other to comprehend the overall meaning of a scene. For example, you’re watching a video of a person jumping and clapping, both visual cues, but the sound is muted. You might realize they’re happy, but you can’t understand why they’re happy without the sound. However, if the sound is unmuted, you could realize they’re cheering for a soccer team that scored a goal.\n",
    "\n",
    "Thus, an application that analyzes a single type of information can’t provide a comprehensive understanding of a video. TwelveLabs’ video understanding models, however, analyze and combine information from all the modalities to accurately interpret the meaning of a video holistically, similar to how humans watch, listen, and read simultaneously to understand videos.\n",
    "\n",
    "Our video understanding models have the ability to identify, analyze, and interpret a variety of elements, including but not limited to the following:\n",
    "| Element | Modality | Example |\n",
    "|---------|----------|---------|\n",
    "| People, including famous individuals | Visual | Michael Jordan, Steve Jobs |\n",
    "| Actions | Visual | Running, dancing, kickboxing |\n",
    "| Objects | Visual | Cars, computers, stadiums |\n",
    "| Animals or pets | Visual | Monkeys, cats, horses |\n",
    "| Nature | Visual | Mountains, lakes, forests |\n",
    "| Text displayed on the screen (OCR) | Visual | License plates, handwritten words, number on a player's jersey |\n",
    "| Brand logos | Visual | Nike, Starbucks, Mercedes |\n",
    "| Shot techniques and effects | Visual | Aerial shots, slow motion, time-lapse |\n",
    "| Counting objects | Visual | Number of people in a crowd, items on a shelf, vehicles in traffic |\n",
    "| Sounds | Audio | Chirping (birds), applause, fireworks popping or exploding |\n",
    "| Human speech | Audio | \"Good morning. How may I help you?\" |\n",
    "| Music | Audio | Ominous music, whistling, lyrics |\n",
    "\n",
    "### Modalities\n",
    "Modalities represent the types of information that the models process and analyze in a video. These modalities are central to both indexing and searching video content.\n",
    "\n",
    "The models support the following modalities: \n",
    "\n",
    "- **Visual**: Analyzes visual content in a video, including actions, objects, events, text (through Optical Character Recognition, or OCR), and brand logos.\n",
    "- **Audio**: Analyzes audio content in a video, including ambient sounds, music, and human speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce269fb",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff9e6c",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a876ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import time\n",
    "from IPython.display import clear_output, HTML, display, Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from opensearchpy import AWSV4SignerAuth, OpenSearch, RequestsHttpConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17eca32",
   "metadata": {},
   "source": [
    "### Configure boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8614f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_REGION = \"us-east-1\" # TODO: Replace with your AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3854e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AWS session\n",
    "session = boto3.Session(profile_name='default') # TODO: Replace with your AWS profile\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = session.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0408083",
   "metadata": {},
   "source": [
    "### Configure S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 Configuration\n",
    "S3_BUCKET_NAME = \"<YOUR_S3_BUCKET_NAME>\" # TODO: Replace with your S3 bucket name\n",
    "S3_VIDEOS_PATH = \"videos\"\n",
    "S3_IMAGES_PATH = \"images\"\n",
    "S3_EMBEDDINGS_PATH = \"embeddings\"\n",
    "\n",
    "# Validate S3 bucket name\n",
    "if S3_BUCKET_NAME == \"<YOUR_S3_BUCKET_NAME>\" or S3_BUCKET_NAME == \"\":\n",
    "    raise ValueError(\"Please replace <YOUR_S3_BUCKET_NAME> with your S3 bucket name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05dd6a5",
   "metadata": {},
   "source": [
    "### Enabling model access on Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0652dbc",
   "metadata": {},
   "source": [
    "## Part 1: Multimodal Embeddings with Marengo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde65ef",
   "metadata": {},
   "source": [
    "### Part 1a: What is an embedding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f27a98",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Use TwelveLabs Marengo to create multimodal embeddings for videos, texts, images, and audio files. These embeddings are contextual vector representations (a series of numbers) that capture interactions between modalities, such as visual expressions, body language, spoken words, and video context. You can apply these embeddings to downstream tasks like training custom multimodal models for anomaly detection, diversity sorting, sentiment analysis, recommendations, or building Retrieval-Augmented Generation (RAG) systems.\n",
    "\n",
    "Key features:\n",
    "- **Native multimodal support**: Process all modalities natively without separate models or frame conversion.\n",
    "- **State-of-the-art performance**: Captures motion and temporal information for accurate video interpretation.\n",
    "- **Unified vector space**: Combines embeddings from different modalities for holistic understanding.\n",
    "- **Fast and reliable**: Reduces processing time for large video sets.\n",
    "- **Flexible segmentation**: Generate embeddings for video segments or the entire video.\n",
    "\n",
    "Use cases:\n",
    "- **Anomaly detection**: Identify unusual patterns, such as corrupt videos with black backgrounds, to improve data set quality.\n",
    "- **Diversity sorting**: Organize data for broad representation, reducing bias and improving AI model training.\n",
    "- **Sentiment analysis**: Combine vocal tone, facial expressions, and spoken language for accurate insights, which particularly useful for customer service.\n",
    "- **Recommendations**: Use embeddings in similarity-based retrieval and ranking systems for recommendations.\n",
    "\n",
    "To learn more about embeddings, check out [The Multimodal Evolution of Vector Embeddings](https://www.twelvelabs.io/blog/multimodal-embeddings) on the TwelveLabs Blog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76298876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embeddings\n",
    "sample_embedding_1 = np.random.rand(1, 1024)\n",
    "sample_embedding_2 = np.random.rand(1, 1024)\n",
    "\n",
    "df_embedding_1 = pd.DataFrame(sample_embedding_1)\n",
    "df_embedding_2 = pd.DataFrame(sample_embedding_2)\n",
    "\n",
    "df_embedding_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbd437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video embedding\n",
    "sample_video_embedding = np.random.rand(5, 1024)\n",
    "df_video_embedding = pd.DataFrame(sample_video_embedding)\n",
    "df_video_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d44fe",
   "metadata": {},
   "source": [
    "### Part 1b: Calculating cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e7a32",
   "metadata": {},
   "source": [
    "Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between them in high-dimensional space. Unlike distance metrics that consider magnitude, cosine similarity focuses purely on the orientation or direction of vectors, making it particularly useful for comparing text embeddings, documents, and other high-dimensional data.\n",
    "\n",
    "The multimodal vector embeddings from TwelveLabs Marengo can be used to calculate the similarity across text, image, audio, and video.\n",
    "\n",
    "***Formula***\n",
    "\n",
    "The cosine similarity between two vectors **A** and **B** is calculated as:\n",
    "\n",
    "```\n",
    "cos(θ) = (A · B) / (||A|| × ||B||)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **A · B** is the dot product of vectors A and B\n",
    "- **||A||** and **||B||** are the magnitudes (norms) of vectors A and B respectively\n",
    "- **θ** is the angle between the two vectors\n",
    "\n",
    "***Key Characteristics***\n",
    "- **Range**: Values range from -1 to 1\n",
    "  - **1**: Identical direction (perfect similarity)\n",
    "  - **0**: Orthogonal vectors (no similarity)\n",
    "  - **-1**: Opposite directions (perfect dissimilarity)\n",
    "- **Magnitude Independence**: Only considers vector direction, not size\n",
    "- **Symmetric**: cos(A,B) = cos(B,A)\n",
    "\n",
    "***Benefits***\n",
    "- **Scale Invariant**: Ideal for comparing vectors of different magnitudes\n",
    "- **Computationally Efficient**: Fast calculation, especially with sparse vectors\n",
    "- **Robust for Text Analysis**: Perfect for document similarity and text embeddings\n",
    "- **Handles High Dimensions**: Works well in high-dimensional spaces without curse of dimensionality issues\n",
    "- **Intuitive Results**: Easy to interpret similarity scores between 0 and 1 for most applications\n",
    "\n",
    "***Drawbacks***\n",
    "- **Ignores Magnitude**: Completely disregards vector size, which may contain important information\n",
    "- **Limited with Negative Values**: Can be less meaningful when dealing with vectors containing negative components\n",
    "- **Not Always Intuitive**: May not align with human perception of similarity in certain domains\n",
    "- **Loses Information**: Discarding magnitude means losing potentially valuable signal strength data\n",
    "- **Poor for Sparse Positive Data**: May not distinguish well between vectors with very few non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15411bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity between two single segment embeddings\n",
    "similarity = cosine_similarity(df_embedding_1, df_embedding_2)\n",
    "pd.DataFrame(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70473e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity with a multi-segment embedding\n",
    "similarities = cosine_similarity(df_video_embedding, df_embedding_1)\n",
    "pd.DataFrame(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bf486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the max similarity and the index of the max similarity\n",
    "max_similarity = np.max(similarities)\n",
    "max_similarity_index = np.argmax(similarities)\n",
    "\n",
    "print(f\"Max similarity: {max_similarity}\")\n",
    "print(f\"Index of max similarity: {max_similarity_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e9437",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Building Multimodal Video Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3cee1",
   "metadata": {},
   "source": [
    "### Part 2a: Storing videos in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86911a29",
   "metadata": {},
   "source": [
    "#### Set up sample dataset to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5714aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Account ID for S3 bucket ownership\n",
    "aws_account_id = session.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"AWS Account ID: {aws_account_id}\")\n",
    "print(f\"S3 Bucket: {S3_BUCKET_NAME}\")\n",
    "print(f\"S3 Videos Path: {S3_VIDEOS_PATH}\")\n",
    "print(f\"S3 Images Path: {S3_IMAGES_PATH}\")\n",
    "print(f\"S3 Embeddings Path: {S3_EMBEDDINGS_PATH}\")\n",
    "\n",
    "# Verify bucket access\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=S3_BUCKET_NAME)\n",
    "    print(f\"✅ Successfully connected to S3 bucket: {S3_BUCKET_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error accessing S3 bucket: {e}\")\n",
    "    print(\"Please ensure the bucket exists and you have proper permissions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c756de",
   "metadata": {},
   "source": [
    "#### Netflix Open Content\n",
    "\n",
    "The [Netflix Open Content](https://opencontent.netflix.com/) is an open source content available under the [Creative Commons Attribution 4.0 International Public License](https://www.google.com/url?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby%2F4.0%2Flegalcode&sa=D&sntz=1&usg=AOvVaw3DDX6ldzWtAO5wOs5KkByf).\n",
    "\n",
    "The assets are available for download at: http://download.opencontent.netflix.com/\n",
    "\n",
    "We will be utilizing a subset of the videos for demonstrating how to utilize the TwelveLabs models on Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video S3 URIs\n",
    "sample_videos = [\n",
    "    # 's3://download.opencontent.netflix.com/TechblogAssets/CosmosLaundromat/encodes/CosmosLaundromat_2048x858_24fps_SDR.mp4',\n",
    "    # 's3://download.opencontent.netflix.com/TechblogAssets/Meridian/encodes/Meridian_3840x2160_5994fps_SDR.mp4',\n",
    "    's3://download.opencontent.netflix.com/TechblogAssets/Sparks/encodes/Sparks_4096x2160_5994fps_SDR.mp4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88871c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsigned S3 client\n",
    "public_s3_client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_s3_uri(s3_uri: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parses an S3 URI like s3://bucket-name/path/to/object and returns (bucket, key)\n",
    "\n",
    "    Args:\n",
    "        s3_uri (str): The S3 URI to parse\n",
    "        \n",
    "    Returns:\n",
    "        tuple[str, str]: The bucket and key\n",
    "    \"\"\"\n",
    "    pattern = r'^s3://([^/]+)/(.+)$'\n",
    "    match = re.match(pattern, s3_uri)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid S3 URI format: {s3_uri}\")\n",
    "    return match.group(1), match.group(2)\n",
    "\n",
    "\n",
    "def copy_public_s3_object_to_private_bucket(public_s3_uri: str, dest_bucket: str, dest_key: str, aws_profile: str = 'default') -> None:\n",
    "    \"\"\"\n",
    "    Copies a public S3 object to a private bucket\n",
    "\n",
    "    Args:\n",
    "        public_s3_uri (str): The S3 URI of the public object to copy\n",
    "        dest_bucket (str): The name of the private bucket to copy to\n",
    "        dest_key (str): The key of the object to copy to\n",
    "        aws_profile (str): The AWS profile to use for the authenticated client\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse source bucket and key\n",
    "    source_bucket, source_key = parse_s3_uri(public_s3_uri)\n",
    "\n",
    "    # Anonymous client to read public object\n",
    "    anon_s3 = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n",
    "\n",
    "    print(f\"Downloading from {public_s3_uri}...\")\n",
    "    response = anon_s3.get_object(Bucket=source_bucket, Key=source_key)\n",
    "    data = response['Body'].read()\n",
    "\n",
    "    print(f\"Uploading to s3://{dest_bucket}/{dest_key} ...\")\n",
    "    s3_client.put_object(Bucket=dest_bucket, Key=dest_key, Body=data)\n",
    "\n",
    "    print(\"✅ Copy completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy videos to the S3 bucket\n",
    "for video_uri in sample_videos:\n",
    "    # Extract the filename from the S3 key\n",
    "    _, src_key = parse_s3_uri(video_uri)\n",
    "    filename = src_key.split(\"/\")[-1]\n",
    "    dest_key = f\"{S3_VIDEOS_PATH}/{filename}\"\n",
    "    copy_public_s3_object_to_private_bucket(\n",
    "        public_s3_uri=video_uri,\n",
    "        dest_bucket=S3_BUCKET_NAME,\n",
    "        dest_key=dest_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d059e",
   "metadata": {},
   "source": [
    "### Part 2b: Creating vector embeddings with Marengo on Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeee87d",
   "metadata": {},
   "source": [
    "#### TwelveLabs Marengo\n",
    "\n",
    "Marengo is an embedding model for comprehensive video understanding. Marengo analyzes multiple modalities in video content, including visuals, audio, and text, to provide a holistic understanding similar to human comprehension.\n",
    "\n",
    "***Key features***\n",
    "- **Multimodal processing:** Combines visual, audio, and text elements for comprehensive understanding\n",
    "- **Fine-grained search:** Detects brand logos, text, and small objects (as small as 10% of the video frame)\n",
    "- **Motion search:** Identifies and analyzes movement within videos\n",
    "- **Counting capabilities:** Accurately counts objects in video frames\n",
    "- **Audio comprehension:** Analyzes music, lyrics, sound, and silence\n",
    "\n",
    "***Use cases***\n",
    "- **Search:** Use natural language queries to find specific content within videos\n",
    "- **Embeddings:** Create video embeddings for various downstream applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9914e4",
   "metadata": {},
   "source": [
    "#### Marengo Embed 2.7 on Bedrock\n",
    "\n",
    "A multimodal embedding model that generates high-quality vector representations of video, text, audio, and image content for similarity search, clustering, and other machine learning tasks. The model supports multiple input modalities and provides specialized embeddings optimized for different use cases.\n",
    "\n",
    "The model supports asynchronous inference through the [StartAsyncInvoke API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_StartAsyncInvoke.html).\n",
    "- Provider — TwelveLabs\n",
    "- Categories — Embeddings, multimodal\n",
    "- Model ID — `twelvelabs.marengo-embed-2-7-v1:0`\n",
    "- Input modality — Video, Text, Audio, Image\n",
    "- Output modality — Embeddings\n",
    "- Max video size — 2 hours long video (< 2GB file size)\n",
    "\n",
    "**Resources:**\n",
    "- [AWS Docs: TwelveLabs Marengo Embed 2.7](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-marengo.html)\n",
    "- [TwelveLabs Docs: Marengo](https://docs.twelvelabs.io/v1.3/docs/concepts/models/marengo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marengo model configuration\n",
    "MODEL_ID = 'twelvelabs.marengo-embed-2-7-v1:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to wait for async embedding results\n",
    "def wait_for_embedding_output(s3_bucket: str, s3_prefix: str, invocation_arn: str, verbose: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Wait for Bedrock async embedding task to complete and retrieve results\n",
    "\n",
    "    Args:\n",
    "        s3_bucket (str): The S3 bucket name\n",
    "        s3_prefix (str): The S3 prefix for the embeddings\n",
    "        invocation_arn (str): The ARN of the Bedrock async embedding task\n",
    "\n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If the embedding task fails or no output.json is found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wait until task completes\n",
    "    status = None\n",
    "    while status not in [\"Completed\", \"Failed\", \"Expired\"]:\n",
    "        response = bedrock_client.get_async_invoke(invocationArn=invocation_arn)\n",
    "        status = response['status']\n",
    "        if verbose:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Embedding task status: {status}\")\n",
    "        time.sleep(5)\n",
    "    \n",
    "    if status != \"Completed\":\n",
    "        raise Exception(f\"Embedding task failed with status: {status}\")\n",
    "    \n",
    "    # Retrieve the output from S3\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_prefix)\n",
    "    \n",
    "    for obj in response.get('Contents', []):\n",
    "        if obj['Key'].endswith('output.json'):\n",
    "            output_key = obj['Key']\n",
    "            obj = s3_client.get_object(Bucket=s3_bucket, Key=output_key)\n",
    "            content = obj['Body'].read().decode('utf-8')\n",
    "            data = json.loads(content).get(\"data\", [])\n",
    "            return data\n",
    "    \n",
    "    raise Exception(\"No output.json found in S3 prefix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9caf4",
   "metadata": {},
   "source": [
    "##### Creating a text embedding with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f68eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text embedding\n",
    "def create_text_embedding(text_query: str) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for text using Marengo on Bedrock\n",
    "\n",
    "    Args:\n",
    "        text_query (str): The text query to create an embedding for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_output_prefix = f'{S3_EMBEDDINGS_PATH}/text/{uuid.uuid4()}'\n",
    "    \n",
    "    response = bedrock_client.start_async_invoke(\n",
    "        modelId=MODEL_ID,\n",
    "        modelInput={\n",
    "            \"inputType\": \"text\",\n",
    "            \"inputText\": text_query\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f's3://{S3_BUCKET_NAME}/{s3_output_prefix}'\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    invocation_arn = response[\"invocationArn\"]\n",
    "    print(f\"Text embedding task started: {invocation_arn}\")\n",
    "    \n",
    "    # Wait for completion and get results\n",
    "    try:\n",
    "        embedding_data = wait_for_embedding_output(S3_BUCKET_NAME, s3_output_prefix, invocation_arn)\n",
    "    except Exception as e:\n",
    "        print(f\"Error waiting for embedding output: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create text embedding\n",
    "text_query = \"two people having a conversation in a car\"\n",
    "\n",
    "print(f\"Creating text embedding for query\")\n",
    "text_embedding_data = create_text_embedding(text_query)\n",
    "\n",
    "print(f\"✅ Text embedding created successfully with {len(text_embedding_data)} segment and {len(text_embedding_data[0]['embedding'])} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdaa16",
   "metadata": {},
   "source": [
    "##### Creating an image embedding with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose image\n",
    "image_path = \"assets/images/image.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload image to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=image_path,\n",
    "    Bucket=S3_BUCKET_NAME,\n",
    "    Key=f\"{S3_IMAGES_PATH}/{image_path.split('/')[-1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image embedding\n",
    "def create_image_embedding(image_s3_uri: str) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for image using Marengo on Bedrock\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the image to create an embedding for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "    \"\"\"\n",
    "\n",
    "    s3_output_prefix = f'{S3_EMBEDDINGS_PATH}/{S3_IMAGES_PATH}/{uuid.uuid4()}'\n",
    "    \n",
    "    response = bedrock_client.start_async_invoke(\n",
    "        modelId=MODEL_ID,\n",
    "        modelInput={\n",
    "            \"inputType\": \"image\",\n",
    "            \"mediaSource\": {\n",
    "                \"s3Location\": {\n",
    "                    \"uri\": image_s3_uri,\n",
    "                    \"bucketOwner\": aws_account_id\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f's3://{S3_BUCKET_NAME}/{s3_output_prefix}'\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    invocation_arn = response[\"invocationArn\"]\n",
    "    print(f\"Text embedding task started: {invocation_arn}\")\n",
    "    \n",
    "    # Wait for completion and get results\n",
    "    try:\n",
    "        embedding_data = wait_for_embedding_output(S3_BUCKET_NAME, s3_output_prefix, invocation_arn)\n",
    "    except Exception as e:\n",
    "        print(f\"Error waiting for embedding output: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5da770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create image embedding\n",
    "image_uri = f\"s3://{S3_BUCKET_NAME}/{S3_IMAGES_PATH}/{image_path.split('/')[-1]}\"\n",
    "\n",
    "print(f\"Creating embeddings for image: {image_uri}\")\n",
    "image_embedding_data = create_image_embedding(image_uri)\n",
    "\n",
    "print(f\"✅ Image embedding created successfully with {len(image_embedding_data)} segment(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f0e9d",
   "metadata": {},
   "source": [
    "##### Creating video embeddings with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2848143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store video embedding to video file mapping\n",
    "video_embedding_mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df07260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create video embedding\n",
    "def create_video_embedding(video_s3_uri: str) -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings for video using Marengo on Bedrock\n",
    "    \n",
    "    Args:\n",
    "        video_s3_uri (str): The S3 URI of the video to create an embedding for\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of embedding data\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_id = uuid.uuid4()\n",
    "    s3_output_prefix = f'{S3_EMBEDDINGS_PATH}/{S3_VIDEOS_PATH}/{unique_id}'\n",
    "    \n",
    "    response = bedrock_client.start_async_invoke(\n",
    "        modelId=MODEL_ID,\n",
    "        modelInput={\n",
    "            \"inputType\": \"video\",\n",
    "            \"mediaSource\": {\n",
    "                \"s3Location\": {\n",
    "                    \"uri\": video_s3_uri,\n",
    "                    \"bucketOwner\": aws_account_id\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        outputDataConfig={\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f's3://{S3_BUCKET_NAME}/{s3_output_prefix}'\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    invocation_arn = response[\"invocationArn\"]\n",
    "    print(f\"Video embedding task started: {invocation_arn}\")\n",
    "    \n",
    "    # Wait for completion and get results\n",
    "    try:\n",
    "        embedding_data = wait_for_embedding_output(S3_BUCKET_NAME, s3_output_prefix, invocation_arn)\n",
    "        video_embedding_mapping[str(unique_id)] = video_s3_uri\n",
    "    except Exception as e:\n",
    "        print(f\"Error waiting for embedding output: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return embedding_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create video embedding\n",
    "videos = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_VIDEOS_PATH)[\"Contents\"]\n",
    "video_uri = f\"s3://{S3_BUCKET_NAME}/{videos[0]['Key']}\"\n",
    "\n",
    "print(f\"Creating embeddings for video: {video_uri}\")\n",
    "video_embedding_data = create_video_embedding(video_uri)\n",
    "\n",
    "print(f\"✅ Video embedding created successfully with {len(video_embedding_data)} segment(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c1e767",
   "metadata": {},
   "source": [
    "### Part 2c: Creating a vector index in OpenSearch Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f4d7b",
   "metadata": {},
   "source": [
    "#### Configure Amazon Opensearch Serverless Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenSearch Serverless configuration\n",
    "OPENSEARCH_ENDPOINT = \"<YOUR_OPENSEARCH_ENDPOINT>\"  # TODO: Replace with your OpenSearch endpoint\n",
    "INDEX_NAME = \"video-embeddings-index\"\n",
    "\n",
    "# Validate OpenSearch endpoint\n",
    "if OPENSEARCH_ENDPOINT == \"<YOUR_OPENSEARCH_ENDPOINT>\" or OPENSEARCH_ENDPOINT == \"\":\n",
    "    raise ValueError(\"Please replace <YOUR_OPENSEARCH_ENDPOINT> with your OpenSearch endpoint\")\n",
    "elif OPENSEARCH_ENDPOINT.startswith(\"https://\"):\n",
    "    OPENSEARCH_ENDPOINT = OPENSEARCH_ENDPOINT.replace(\"https://\", \"\")\n",
    "\n",
    "# Create OpenSearch client for Amazon OpenSearch Serverless\n",
    "service = \"aoss\"\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, AWS_REGION, service)\n",
    "\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{\"host\": OPENSEARCH_ENDPOINT, \"port\": 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    pool_maxsize=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8336c8",
   "metadata": {},
   "source": [
    "#### Create a new index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenSearch vector index\n",
    "def create_opensearch_index(os_client: OpenSearch, index_name: str):\n",
    "    \"\"\"\n",
    "    Create a vector index in OpenSearch for storing video embeddings\n",
    "\n",
    "    Args:\n",
    "        os_client (OpenSearch): The OpenSearch client\n",
    "        index_name (str): The name of the index to create\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    if os_client.indices.exists(index=index_name):\n",
    "        print(f\"Index '{index_name}' already exists.\")\n",
    "        return\n",
    "    \n",
    "    index_body = {\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"knn\": True,\n",
    "                \"number_of_shards\": 1,\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 1024,\n",
    "                    \"method\": {\n",
    "                        \"engine\": \"faiss\",\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                    },\n",
    "                },\n",
    "                \"start_time\": {\"type\": \"float\"},\n",
    "                \"end_time\": {\"type\": \"float\"},\n",
    "                \"video_id\": {\"type\": \"keyword\"},\n",
    "                \"segment_text\": {\"type\": \"text\"},\n",
    "                \"embedding_option\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    os_client.indices.create(index=index_name, body=index_body)\n",
    "    print(f\"✅ Index '{index_name}' created successfully.\")\n",
    "\n",
    "# Create the index\n",
    "create_opensearch_index(os_client, INDEX_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e50715",
   "metadata": {},
   "source": [
    "#### Bulk process videos in S3 with Marengo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9fb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index video embeddings in OpenSearch\n",
    "def index_video_embeddings(os_client: OpenSearch, index_name: str, video_embeddings: list, video_id: str = \"sample_video\") -> int:\n",
    "    \"\"\"\n",
    "    Index video embeddings into OpenSearch\n",
    "    \n",
    "    Args:\n",
    "        os_client (OpenSearch): The OpenSearch client\n",
    "        index_name (str): The name of the index to create\n",
    "        video_embeddings (list): The list of video embeddings\n",
    "        video_id (str): The id of the video\n",
    "\n",
    "    Returns:\n",
    "        int: The number of documents indexed\n",
    "    \"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for i, segment in enumerate(video_embeddings):\n",
    "        document = {\n",
    "            \"embedding\": segment[\"embedding\"],\n",
    "            \"start_time\": segment[\"startSec\"],\n",
    "            \"end_time\": segment[\"endSec\"],\n",
    "            \"video_id\": video_id,\n",
    "            \"segment_id\": i,\n",
    "            \"embedding_option\": segment.get(\"embeddingOption\", \"visual-text\")\n",
    "        }\n",
    "        documents.append(document)\n",
    "    \n",
    "    # Bulk index documents\n",
    "    bulk_data = []\n",
    "    for doc in documents:\n",
    "        bulk_data.append({\"index\": {\"_index\": index_name}})\n",
    "        bulk_data.append(doc)\n",
    "    \n",
    "    # Convert to bulk format\n",
    "    bulk_body = \"\\n\".join(json.dumps(item) for item in bulk_data) + \"\\n\"\n",
    "    \n",
    "    response = os_client.bulk(body=bulk_body, index=index_name)\n",
    "    \n",
    "    if response[\"errors\"]:\n",
    "        print(\"Some documents failed to index:\")\n",
    "        for item in response[\"items\"]:\n",
    "            if \"index\" in item and \"error\" in item[\"index\"]:\n",
    "                print(f\"Error: {item['index']['error']}\")\n",
    "    \n",
    "    return len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of videos in the s3 bucket and loop through them to create embeddings\n",
    "videos = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=S3_VIDEOS_PATH)[\"Contents\"]\n",
    "\n",
    "for video in videos:\n",
    "    video_uri = f\"s3://{S3_BUCKET_NAME}/{video['Key']}\"\n",
    "    print(f\"Creating embeddings for video: {video_uri}\")\n",
    "    video_embedding_data = create_video_embedding(video_uri)\n",
    "\n",
    "    print(f\"✅ Video embedding created successfully with {len(video_embedding_data)} segment(s) from {video['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca80cc",
   "metadata": {},
   "source": [
    "#### Insert embeddings into OpenSearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aad509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of embedding files in the S3 bucket\n",
    "embedding_files = s3_client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=f\"{S3_EMBEDDINGS_PATH}/{S3_VIDEOS_PATH}\").get(\"Contents\", [])\n",
    "\n",
    "for embedding_file in embedding_files:\n",
    "    embedding_key = embedding_file[\"Key\"]\n",
    "    if not embedding_key.endswith(\"output.json\"):\n",
    "        continue  # Skip non-JSON files\n",
    "\n",
    "    embedding_obj = s3_client.get_object(Bucket=S3_BUCKET_NAME, Key=embedding_key)\n",
    "    content = embedding_obj['Body'].read().decode('utf-8')\n",
    "    embedding_data = json.loads(content).get(\"data\", [])\n",
    "\n",
    "    # Use the index_video_embeddings function to index the embedding data into OpenSearch\n",
    "    num_indexed = index_video_embeddings(os_client, INDEX_NAME, embedding_data, video_id=embedding_key.split(\"/\")[2])\n",
    "\n",
    "    print(f\"✅ Indexed {num_indexed} segments from {embedding_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4579de4c",
   "metadata": {},
   "source": [
    "### Part 2d: Querying for multimodal video search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28524771",
   "metadata": {},
   "source": [
    "#### Query with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651d7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Query Search Function\n",
    "def search_videos_by_text(query_text: str, top_k: int=5) -> list:\n",
    "    \"\"\"\n",
    "    Search for video segments using text queries\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The text query to search for.\n",
    "        top_k (int): The number of videos to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of video segments that match the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate embedding for the text query\n",
    "    print(f\"Generating embedding for query: '{query_text}'\")\n",
    "    query_embedding_data = create_text_embedding(query_text)\n",
    "    query_embedding = query_embedding_data[0][\"embedding\"]\n",
    "    \n",
    "    # Search OpenSearch index\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": top_k\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "        \"_source\": [\"start_time\", \"end_time\", \"video_id\", \"segment_id\"]\n",
    "    }\n",
    "    \n",
    "    response = os_client.search(index=INDEX_NAME, body=search_body)\n",
    "    \n",
    "    print(f\"\\n✅ Found {len(response['hits']['hits'])} matching segments:\")\n",
    "    results = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"video_id\": hit[\"_source\"][\"video_id\"],\n",
    "            \"segment_id\": hit[\"_source\"][\"segment_id\"],\n",
    "            \"start_time\": hit[\"_source\"][\"start_time\"],\n",
    "            \"end_time\": hit[\"_source\"][\"end_time\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Score: {result['score']:.4f} | Video: {video_embedding_mapping[result['video_id']]} | \"\n",
    "              f\"Segment: {result['segment_id']} | Time: {result['start_time']:.1f}s - {result['end_time']:.1f}s\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"a person wearing safety gear and welding with a forest in the background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text search\n",
    "text_search_results = search_videos_by_text(text_query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top result\n",
    "top_text_result = text_search_results[0]\n",
    "video_bucket, video_key = parse_s3_uri(video_embedding_mapping[top_text_result[\"video_id\"]])\n",
    "\n",
    "# Generate presigned URL for the video\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    \"get_object\",\n",
    "    Params={\"Bucket\": video_bucket, \"Key\": video_key},\n",
    "    ExpiresIn=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c6671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the video stream URL and the start time\n",
    "video_url = presigned_url\n",
    "start_time = top_text_result[\"start_time\"]\n",
    "print(f\"\\nVideo URL: {video_url}\")\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "# Play the video\n",
    "html_code = f\"\"\"\n",
    "<video width=\"640\" controls>\n",
    "  <source src=\"{video_url}#t={start_time}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "display(HTML(html_code))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a624dbd",
   "metadata": {},
   "source": [
    "#### Query with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Query Search Function\n",
    "def search_videos_by_image(image_path: str, top_k: int=5) -> list:\n",
    "    \"\"\"\n",
    "    Search for videos that contain the given image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image to search for.\n",
    "        top_k (int): The number of videos to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of video segments that match the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Upload image to S3\n",
    "    s3_client.upload_file(\n",
    "        Filename=image_path,\n",
    "        Bucket=S3_BUCKET_NAME,\n",
    "        Key=f\"{S3_IMAGES_PATH}/{image_path.split('/')[-1]}\"\n",
    "    )\n",
    "\n",
    "    # Create image embedding\n",
    "    print(f\"Creating embeddings for image: {image_path}\")\n",
    "    embedding_data = create_image_embedding(f\"s3://{S3_BUCKET_NAME}/{S3_IMAGES_PATH}/{image_path.split('/')[-1]}\")\n",
    "    query_embedding = embedding_data[0][\"embedding\"]\n",
    "\n",
    "    # Search OpenSearch index\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": top_k\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": top_k,\n",
    "        \"_source\": [\"start_time\", \"end_time\", \"video_id\", \"segment_id\"]\n",
    "    }\n",
    "    \n",
    "    response = os_client.search(index=INDEX_NAME, body=search_body)\n",
    "    \n",
    "    print(f\"\\n✅ Found {len(response['hits']['hits'])} matching segments:\")\n",
    "    results = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result = {\n",
    "            \"score\": hit[\"_score\"],\n",
    "            \"video_id\": hit[\"_source\"][\"video_id\"],\n",
    "            \"segment_id\": hit[\"_source\"][\"segment_id\"],\n",
    "            \"start_time\": hit[\"_source\"][\"start_time\"],\n",
    "            \"end_time\": hit[\"_source\"][\"end_time\"]\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Score: {result['score']:.4f} | Video: {video_embedding_mapping[result['video_id']]} | \"\n",
    "              f\"Segment: {result['segment_id']} | Time: {result['start_time']:.1f}s - {result['end_time']:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01730b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_query = \"assets/images/image.jpg\"\n",
    "\n",
    "display(Image(filename=image_query, width=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8653813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image search\n",
    "image_search_results = search_videos_by_image(image_path=image_query, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41764363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View top result\n",
    "top_image_result = image_search_results[0]\n",
    "video_bucket, video_key = parse_s3_uri(video_embedding_mapping[top_image_result[\"video_id\"]])\n",
    "\n",
    "# Generate presigned URL for the video\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    \"get_object\",\n",
    "    Params={\"Bucket\": video_bucket, \"Key\": video_key},\n",
    "    ExpiresIn=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c573fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the video stream URL and the start time\n",
    "video_url = presigned_url\n",
    "start_time = top_image_result[\"start_time\"]\n",
    "print(f\"\\nVideo URL: {video_url}\")\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "# Play the video\n",
    "html_code = f\"\"\"\n",
    "<video width=\"640\" controls>\n",
    "  <source src=\"{video_url}#t={start_time}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfcc23",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2a4ad",
   "metadata": {},
   "source": [
    "### Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959407b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
